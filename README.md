# nanogpt

The code is a small replication of the GPT-2 model, which is a popular transformer-based language model. The code is written in Python and uses the PyTorch library for training and inference. The code is designed to be easy to understand and modify, making it a good starting point for my learning about transformer models and natural language processing toward my 10,000 hour goal in NNs.

### License
MIT